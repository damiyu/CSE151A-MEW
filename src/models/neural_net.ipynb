{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset here\n",
    "file = open(\"../dataset/processed_reviews.json\", 'r', encoding='utf8')\n",
    "dataset_dict = json.load(file)\n",
    "df_raw = pd.DataFrame(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused features\n",
    "df = df_raw.copy(deep=True) #Do this so that I dont have to rerun the previous cell every time I make a change\n",
    "df.drop(columns=['firm','job_title'], inplace=True) #one hotting these would create too many features\n",
    "\n",
    "# Split up Date\n",
    "df['date'] = pd.to_datetime(df['date_review'])\n",
    "df['month'] = df['date'].dt.month.astype(str)\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Consider the length text inputs\n",
    "df['pros_length'] = df['pros'].apply(lambda x: len(x))\n",
    "df['cons_length'] = df['cons'].apply(lambda x: len(x))\n",
    "df.drop(columns=['headline', 'pros', 'cons'], inplace=True)\n",
    "\n",
    "# Encode 'current' as int\n",
    "df['current'] = (df['current'] == 'Current Employee').astype(int)\n",
    "\n",
    "# Min-max normalization\n",
    "scaler = MinMaxScaler()\n",
    "numeric_cols = df.select_dtypes(include=['int', 'float']).columns\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "#One hot encode\n",
    "one_hot_encoded = pd.get_dummies(df[['recommend', 'ceo_approv', 'outlook', 'month', 'duration']])\n",
    "df = pd.concat([df, one_hot_encoded], axis=1)\n",
    "df.drop(columns=['date', 'date_review', 'recommend', 'ceo_approv', 'outlook', 'month', 'duration'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['current', 'overall_rating', 'work_life_balance', 'culture_values',\n",
      "       'career_opp', 'comp_benefits', 'senior_mgmt', 'year', 'pros_length',\n",
      "       'cons_length', 'recommend_o', 'recommend_v', 'recommend_x',\n",
      "       'ceo_approv_o', 'ceo_approv_r', 'ceo_approv_v', 'ceo_approv_x',\n",
      "       'outlook_o', 'outlook_r', 'outlook_v', 'outlook_x', 'month_1',\n",
      "       'month_10', 'month_11', 'month_12', 'month_2', 'month_3', 'month_4',\n",
      "       'month_5', 'month_6', 'month_7', 'month_8', 'month_9',\n",
      "       'duration_less than 1 year', 'duration_more than 1 year',\n",
      "       'duration_more than 10 years', 'duration_more than 3 years',\n",
      "       'duration_more than 5 years', 'duration_more than 8 years',\n",
      "       'duration_not mentioned'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>work_life_balance</th>\n",
       "      <th>culture_values</th>\n",
       "      <th>career_opp</th>\n",
       "      <th>comp_benefits</th>\n",
       "      <th>senior_mgmt</th>\n",
       "      <th>year</th>\n",
       "      <th>pros_length</th>\n",
       "      <th>cons_length</th>\n",
       "      <th>...</th>\n",
       "      <th>month_7</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>duration_less than 1 year</th>\n",
       "      <th>duration_more than 1 year</th>\n",
       "      <th>duration_more than 10 years</th>\n",
       "      <th>duration_more than 3 years</th>\n",
       "      <th>duration_more than 5 years</th>\n",
       "      <th>duration_more than 8 years</th>\n",
       "      <th>duration_not mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.011246</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>0.020727</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.020541</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.006607</td>\n",
       "      <td>0.009573</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.005726</td>\n",
       "      <td>0.022493</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   current  overall_rating  work_life_balance  culture_values  career_opp  \\\n",
       "0      1.0            0.25               0.50            0.00        0.25   \n",
       "1      1.0            0.00               0.00            0.00        0.00   \n",
       "2      1.0            0.00               0.25            0.00        0.25   \n",
       "3      1.0            0.50               0.75            0.25        0.25   \n",
       "4      0.0            0.00               0.00            0.00        0.00   \n",
       "\n",
       "   comp_benefits  senior_mgmt      year  pros_length  cons_length  ...  \\\n",
       "0            0.0         0.75  0.538462     0.002257     0.011246  ...   \n",
       "1            0.0         0.00  0.615385     0.002753     0.020727  ...   \n",
       "2            0.0         0.00  0.615385     0.001817     0.020541  ...   \n",
       "3            0.5         0.25  0.615385     0.006607     0.009573  ...   \n",
       "4            0.0         0.00  0.615385     0.005726     0.022493  ...   \n",
       "\n",
       "   month_7  month_8  month_9  duration_less than 1 year  \\\n",
       "0    False    False    False                      False   \n",
       "1    False    False    False                       True   \n",
       "2    False    False    False                      False   \n",
       "3    False    False    False                       True   \n",
       "4    False    False     True                      False   \n",
       "\n",
       "   duration_more than 1 year  duration_more than 10 years  \\\n",
       "0                       True                        False   \n",
       "1                      False                        False   \n",
       "2                       True                        False   \n",
       "3                      False                        False   \n",
       "4                      False                        False   \n",
       "\n",
       "   duration_more than 3 years  duration_more than 5 years  \\\n",
       "0                       False                       False   \n",
       "1                       False                       False   \n",
       "2                       False                       False   \n",
       "3                       False                       False   \n",
       "4                       False                       False   \n",
       "\n",
       "   duration_more than 8 years  duration_not mentioned  \n",
       "0                       False                   False  \n",
       "1                       False                   False  \n",
       "2                       False                   False  \n",
       "3                       False                   False  \n",
       "4                       False                    True  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (681651, 39) (681651,)\n",
      "Testing set shape: (75740, 39) (75740,)\n"
     ]
    }
   ],
   "source": [
    "#Spliting the data\n",
    "X = df.drop(columns=['overall_rating'])\n",
    "y = df['overall_rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    "    start_from_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.1010 - MSE: 0.1010 - val_loss: 0.0892 - val_MSE: 0.0892\n",
      "Epoch 2/200\n",
      "682/682 [==============================] - 1s 984us/step - loss: 0.0848 - MSE: 0.0848 - val_loss: 0.0798 - val_MSE: 0.0798\n",
      "Epoch 3/200\n",
      "682/682 [==============================] - 1s 965us/step - loss: 0.0742 - MSE: 0.0742 - val_loss: 0.0670 - val_MSE: 0.0670\n",
      "Epoch 4/200\n",
      "682/682 [==============================] - 1s 947us/step - loss: 0.0600 - MSE: 0.0600 - val_loss: 0.0536 - val_MSE: 0.0536\n",
      "Epoch 5/200\n",
      "682/682 [==============================] - 1s 953us/step - loss: 0.0502 - MSE: 0.0502 - val_loss: 0.0473 - val_MSE: 0.0473\n",
      "Epoch 6/200\n",
      "682/682 [==============================] - 1s 953us/step - loss: 0.0456 - MSE: 0.0456 - val_loss: 0.0441 - val_MSE: 0.0441\n",
      "Epoch 7/200\n",
      "682/682 [==============================] - 1s 953us/step - loss: 0.0430 - MSE: 0.0430 - val_loss: 0.0420 - val_MSE: 0.0420\n",
      "Epoch 8/200\n",
      "682/682 [==============================] - 1s 959us/step - loss: 0.0411 - MSE: 0.0411 - val_loss: 0.0404 - val_MSE: 0.0404\n",
      "Epoch 9/200\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.0396 - MSE: 0.0396 - val_loss: 0.0390 - val_MSE: 0.0390\n",
      "Epoch 10/200\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.0384 - MSE: 0.0384 - val_loss: 0.0379 - val_MSE: 0.0379\n",
      "Epoch 11/200\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.0373 - MSE: 0.0373 - val_loss: 0.0369 - val_MSE: 0.0369\n",
      "Epoch 12/200\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.0363 - MSE: 0.0363 - val_loss: 0.0360 - val_MSE: 0.0360\n",
      "Epoch 13/200\n",
      "682/682 [==============================] - 1s 961us/step - loss: 0.0355 - MSE: 0.0355 - val_loss: 0.0352 - val_MSE: 0.0352\n",
      "Epoch 14/200\n",
      "682/682 [==============================] - 1s 955us/step - loss: 0.0348 - MSE: 0.0348 - val_loss: 0.0346 - val_MSE: 0.0346\n",
      "Epoch 15/200\n",
      "682/682 [==============================] - 1s 961us/step - loss: 0.0342 - MSE: 0.0342 - val_loss: 0.0340 - val_MSE: 0.0340\n",
      "Epoch 16/200\n",
      "682/682 [==============================] - 1s 950us/step - loss: 0.0336 - MSE: 0.0336 - val_loss: 0.0335 - val_MSE: 0.0335\n",
      "Epoch 17/200\n",
      "682/682 [==============================] - 1s 972us/step - loss: 0.0332 - MSE: 0.0332 - val_loss: 0.0332 - val_MSE: 0.0332\n",
      "Epoch 18/200\n",
      "682/682 [==============================] - 1s 965us/step - loss: 0.0328 - MSE: 0.0328 - val_loss: 0.0328 - val_MSE: 0.0328\n",
      "Epoch 19/200\n",
      "682/682 [==============================] - 1s 964us/step - loss: 0.0325 - MSE: 0.0325 - val_loss: 0.0326 - val_MSE: 0.0326\n",
      "Epoch 20/200\n",
      "682/682 [==============================] - 1s 955us/step - loss: 0.0323 - MSE: 0.0323 - val_loss: 0.0323 - val_MSE: 0.0323\n",
      "Epoch 21/200\n",
      "682/682 [==============================] - 1s 952us/step - loss: 0.0320 - MSE: 0.0320 - val_loss: 0.0321 - val_MSE: 0.0321\n",
      "Epoch 22/200\n",
      "682/682 [==============================] - 1s 963us/step - loss: 0.0319 - MSE: 0.0319 - val_loss: 0.0320 - val_MSE: 0.0320\n",
      "Epoch 23/200\n",
      "682/682 [==============================] - 1s 958us/step - loss: 0.0317 - MSE: 0.0317 - val_loss: 0.0318 - val_MSE: 0.0318\n",
      "Epoch 24/200\n",
      "682/682 [==============================] - 1s 972us/step - loss: 0.0316 - MSE: 0.0316 - val_loss: 0.0317 - val_MSE: 0.0317\n",
      "Epoch 25/200\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.0315 - MSE: 0.0315 - val_loss: 0.0316 - val_MSE: 0.0316\n",
      "Epoch 26/200\n",
      "682/682 [==============================] - 1s 946us/step - loss: 0.0314 - MSE: 0.0314 - val_loss: 0.0315 - val_MSE: 0.0315\n",
      "Epoch 27/200\n",
      "682/682 [==============================] - 1s 965us/step - loss: 0.0313 - MSE: 0.0313 - val_loss: 0.0314 - val_MSE: 0.0314\n",
      "Epoch 28/200\n",
      "682/682 [==============================] - 1s 949us/step - loss: 0.0312 - MSE: 0.0312 - val_loss: 0.0314 - val_MSE: 0.0314\n",
      "Epoch 29/200\n",
      "682/682 [==============================] - 1s 950us/step - loss: 0.0311 - MSE: 0.0311 - val_loss: 0.0313 - val_MSE: 0.0313\n",
      "Epoch 30/200\n",
      "682/682 [==============================] - 1s 977us/step - loss: 0.0311 - MSE: 0.0311 - val_loss: 0.0312 - val_MSE: 0.0312\n",
      "Epoch 31/200\n",
      "682/682 [==============================] - 1s 955us/step - loss: 0.0310 - MSE: 0.0310 - val_loss: 0.0312 - val_MSE: 0.0312\n",
      "Epoch 32/200\n",
      "682/682 [==============================] - 1s 958us/step - loss: 0.0310 - MSE: 0.0310 - val_loss: 0.0311 - val_MSE: 0.0311\n",
      "Epoch 33/200\n",
      "682/682 [==============================] - 1s 955us/step - loss: 0.0309 - MSE: 0.0309 - val_loss: 0.0311 - val_MSE: 0.0311\n",
      "Epoch 34/200\n",
      "682/682 [==============================] - 1s 955us/step - loss: 0.0309 - MSE: 0.0309 - val_loss: 0.0311 - val_MSE: 0.0311\n",
      "Epoch 35/200\n",
      "682/682 [==============================] - 1s 953us/step - loss: 0.0308 - MSE: 0.0308 - val_loss: 0.0310 - val_MSE: 0.0310\n",
      "Epoch 36/200\n",
      "682/682 [==============================] - 1s 940us/step - loss: 0.0308 - MSE: 0.0308 - val_loss: 0.0310 - val_MSE: 0.0310\n",
      "Epoch 37/200\n",
      "682/682 [==============================] - 1s 953us/step - loss: 0.0308 - MSE: 0.0308 - val_loss: 0.0310 - val_MSE: 0.0310\n",
      "Epoch 38/200\n",
      "682/682 [==============================] - 1s 964us/step - loss: 0.0307 - MSE: 0.0307 - val_loss: 0.0309 - val_MSE: 0.0309\n",
      "Epoch 39/200\n",
      "682/682 [==============================] - 1s 950us/step - loss: 0.0307 - MSE: 0.0307 - val_loss: 0.0309 - val_MSE: 0.0309\n",
      "Epoch 40/200\n",
      "682/682 [==============================] - 1s 975us/step - loss: 0.0307 - MSE: 0.0307 - val_loss: 0.0309 - val_MSE: 0.0309\n",
      "Epoch 41/200\n",
      "682/682 [==============================] - 1s 958us/step - loss: 0.0307 - MSE: 0.0307 - val_loss: 0.0309 - val_MSE: 0.0309\n",
      "Epoch 42/200\n",
      "682/682 [==============================] - 1s 956us/step - loss: 0.0306 - MSE: 0.0306 - val_loss: 0.0308 - val_MSE: 0.0308\n",
      "Epoch 43/200\n",
      "682/682 [==============================] - 1s 986us/step - loss: 0.0306 - MSE: 0.0306 - val_loss: 0.0308 - val_MSE: 0.0308\n",
      "Epoch 44/200\n",
      "682/682 [==============================] - 1s 969us/step - loss: 0.0306 - MSE: 0.0306 - val_loss: 0.0308 - val_MSE: 0.0308\n",
      "Epoch 45/200\n",
      "682/682 [==============================] - 1s 977us/step - loss: 0.0306 - MSE: 0.0306 - val_loss: 0.0308 - val_MSE: 0.0308\n",
      "Epoch 46/200\n",
      "682/682 [==============================] - 1s 962us/step - loss: 0.0305 - MSE: 0.0305 - val_loss: 0.0308 - val_MSE: 0.0308\n",
      "Epoch 47/200\n",
      "682/682 [==============================] - 1s 962us/step - loss: 0.0305 - MSE: 0.0305 - val_loss: 0.0307 - val_MSE: 0.0307\n",
      "Epoch 48/200\n",
      "682/682 [==============================] - 1s 981us/step - loss: 0.0305 - MSE: 0.0305 - val_loss: 0.0307 - val_MSE: 0.0307\n",
      "Epoch 49/200\n",
      "682/682 [==============================] - 1s 958us/step - loss: 0.0305 - MSE: 0.0305 - val_loss: 0.0307 - val_MSE: 0.0307\n",
      "Epoch 50/200\n",
      "682/682 [==============================] - 1s 975us/step - loss: 0.0305 - MSE: 0.0305 - val_loss: 0.0307 - val_MSE: 0.0307\n",
      "Epoch 51/200\n",
      "682/682 [==============================] - 1s 984us/step - loss: 0.0305 - MSE: 0.0305 - val_loss: 0.0307 - val_MSE: 0.0307\n",
      "Epoch 52/200\n",
      "682/682 [==============================] - 1s 962us/step - loss: 0.0305 - MSE: 0.0305 - val_loss: 0.0307 - val_MSE: 0.0307\n",
      "Epoch 53/200\n",
      "682/682 [==============================] - 1s 964us/step - loss: 0.0304 - MSE: 0.0304 - val_loss: 0.0306 - val_MSE: 0.0306\n",
      "Epoch 54/200\n",
      "682/682 [==============================] - 1s 968us/step - loss: 0.0304 - MSE: 0.0304 - val_loss: 0.0306 - val_MSE: 0.0306\n",
      "Epoch 55/200\n",
      "682/682 [==============================] - 1s 955us/step - loss: 0.0304 - MSE: 0.0304 - val_loss: 0.0306 - val_MSE: 0.0306\n",
      "Epoch 56/200\n",
      "682/682 [==============================] - 1s 962us/step - loss: 0.0304 - MSE: 0.0304 - val_loss: 0.0306 - val_MSE: 0.0306\n",
      "Epoch 57/200\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.0304 - MSE: 0.0304 - val_loss: 0.0306 - val_MSE: 0.0306\n",
      "Epoch 58/200\n",
      "682/682 [==============================] - 1s 975us/step - loss: 0.0304 - MSE: 0.0304 - val_loss: 0.0306 - val_MSE: 0.0306\n",
      "Epoch 59/200\n",
      "682/682 [==============================] - 1s 965us/step - loss: 0.0304 - MSE: 0.0304 - val_loss: 0.0306 - val_MSE: 0.0306\n",
      "Epoch 60/200\n",
      "682/682 [==============================] - 1s 972us/step - loss: 0.0303 - MSE: 0.0303 - val_loss: 0.0306 - val_MSE: 0.0306\n",
      "Epoch 61/200\n",
      "682/682 [==============================] - 1s 961us/step - loss: 0.0303 - MSE: 0.0303 - val_loss: 0.0305 - val_MSE: 0.0305\n",
      "Epoch 62/200\n",
      "682/682 [==============================] - 1s 972us/step - loss: 0.0303 - MSE: 0.0303 - val_loss: 0.0305 - val_MSE: 0.0305\n",
      "Epoch 63/200\n",
      "682/682 [==============================] - 1s 949us/step - loss: 0.0303 - MSE: 0.0303 - val_loss: 0.0305 - val_MSE: 0.0305\n",
      "Epoch 64/200\n",
      "682/682 [==============================] - 1s 962us/step - loss: 0.0303 - MSE: 0.0303 - val_loss: 0.0305 - val_MSE: 0.0305\n",
      "Epoch 65/200\n",
      "682/682 [==============================] - 1s 950us/step - loss: 0.0303 - MSE: 0.0303 - val_loss: 0.0305 - val_MSE: 0.0305\n",
      "Epoch 66/200\n",
      "682/682 [==============================] - 1s 947us/step - loss: 0.0303 - MSE: 0.0303 - val_loss: 0.0305 - val_MSE: 0.0305\n",
      "Epoch 67/200\n",
      "682/682 [==============================] - 1s 969us/step - loss: 0.0303 - MSE: 0.0303 - val_loss: 0.0305 - val_MSE: 0.0305\n",
      "Epoch 68/200\n",
      "682/682 [==============================] - 1s 953us/step - loss: 0.0303 - MSE: 0.0303 - val_loss: 0.0305 - val_MSE: 0.0305\n",
      "Epoch 69/200\n",
      "682/682 [==============================] - 1s 974us/step - loss: 0.0303 - MSE: 0.0303 - val_loss: 0.0305 - val_MSE: 0.0305\n",
      "Epoch 70/200\n",
      "682/682 [==============================] - 1s 987us/step - loss: 0.0302 - MSE: 0.0302 - val_loss: 0.0304 - val_MSE: 0.0304\n",
      "Epoch 71/200\n",
      "682/682 [==============================] - 1s 968us/step - loss: 0.0302 - MSE: 0.0302 - val_loss: 0.0304 - val_MSE: 0.0304\n",
      "Epoch 72/200\n",
      "682/682 [==============================] - 1s 964us/step - loss: 0.0302 - MSE: 0.0302 - val_loss: 0.0304 - val_MSE: 0.0304\n",
      "Epoch 73/200\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.0302 - MSE: 0.0302 - val_loss: 0.0304 - val_MSE: 0.0304\n",
      "Epoch 74/200\n",
      "682/682 [==============================] - 1s 961us/step - loss: 0.0302 - MSE: 0.0302 - val_loss: 0.0304 - val_MSE: 0.0304\n",
      "Epoch 75/200\n",
      "682/682 [==============================] - 1s 968us/step - loss: 0.0302 - MSE: 0.0302 - val_loss: 0.0304 - val_MSE: 0.0304\n",
      "Epoch 76/200\n",
      "682/682 [==============================] - 1s 965us/step - loss: 0.0302 - MSE: 0.0302 - val_loss: 0.0304 - val_MSE: 0.0304\n",
      "Epoch 77/200\n",
      "682/682 [==============================] - 1s 972us/step - loss: 0.0302 - MSE: 0.0302 - val_loss: 0.0304 - val_MSE: 0.0304\n",
      "Epoch 78/200\n",
      "682/682 [==============================] - 1s 973us/step - loss: 0.0302 - MSE: 0.0302 - val_loss: 0.0304 - val_MSE: 0.0304\n",
      "Epoch 79/200\n",
      "682/682 [==============================] - 1s 960us/step - loss: 0.0302 - MSE: 0.0302 - val_loss: 0.0304 - val_MSE: 0.0304\n",
      "Epoch 80/200\n",
      "682/682 [==============================] - 1s 965us/step - loss: 0.0302 - MSE: 0.0302 - val_loss: 0.0304 - val_MSE: 0.0304\n",
      "Epoch 81/200\n",
      "682/682 [==============================] - 1s 969us/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0303 - val_MSE: 0.0303\n",
      "Epoch 82/200\n",
      "682/682 [==============================] - 1s 978us/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0303 - val_MSE: 0.0303\n",
      "Epoch 83/200\n",
      "682/682 [==============================] - 1s 984us/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0303 - val_MSE: 0.0303\n",
      "Epoch 84/200\n",
      "682/682 [==============================] - 1s 969us/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0303 - val_MSE: 0.0303\n",
      "Epoch 85/200\n",
      "682/682 [==============================] - 1s 988us/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0303 - val_MSE: 0.0303\n",
      "Epoch 86/200\n",
      "682/682 [==============================] - 1s 946us/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0303 - val_MSE: 0.0303\n",
      "Epoch 87/200\n",
      "682/682 [==============================] - 1s 974us/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0303 - val_MSE: 0.0303\n",
      "Epoch 88/200\n",
      "682/682 [==============================] - 1s 966us/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0303 - val_MSE: 0.0303\n",
      "Epoch 89/200\n",
      "682/682 [==============================] - 1s 965us/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0303 - val_MSE: 0.0303\n",
      "Epoch 90/200\n",
      "682/682 [==============================] - 1s 965us/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0303 - val_MSE: 0.0303\n",
      "Epoch 91/200\n",
      "682/682 [==============================] - 1s 962us/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0303 - val_MSE: 0.0303\n",
      "Epoch 92/200\n",
      "682/682 [==============================] - 1s 946us/step - loss: 0.0301 - MSE: 0.0301 - val_loss: 0.0303 - val_MSE: 0.0303\n",
      "Epoch 93/200\n",
      "682/682 [==============================] - 1s 977us/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 94/200\n",
      "682/682 [==============================] - 1s 972us/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 95/200\n",
      "682/682 [==============================] - 1s 968us/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 96/200\n",
      "682/682 [==============================] - 1s 958us/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 97/200\n",
      "682/682 [==============================] - 1s 968us/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 98/200\n",
      "682/682 [==============================] - 1s 965us/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 99/200\n",
      "682/682 [==============================] - 1s 987us/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 100/200\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 101/200\n",
      "682/682 [==============================] - 1s 1ms/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 102/200\n",
      "682/682 [==============================] - 1s 961us/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 103/200\n",
      "682/682 [==============================] - 1s 947us/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 104/200\n",
      "682/682 [==============================] - 1s 969us/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 105/200\n",
      "682/682 [==============================] - 1s 966us/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "Epoch 106/200\n",
      "682/682 [==============================] - 1s 956us/step - loss: 0.0300 - MSE: 0.0300 - val_loss: 0.0302 - val_MSE: 0.0302\n",
      "682/682 [==============================] - 0s 610us/step\n",
      "76/76 [==============================] - 0s 560us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.241834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.488427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.653242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.942512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.297351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75735</th>\n",
       "      <td>0.690084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75736</th>\n",
       "      <td>0.928053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75737</th>\n",
       "      <td>0.914464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75738</th>\n",
       "      <td>0.943409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75739</th>\n",
       "      <td>0.941672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75740 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0      0.241834\n",
       "1      0.488427\n",
       "2      0.653242\n",
       "3      0.942512\n",
       "4      0.297351\n",
       "...         ...\n",
       "75735  0.690084\n",
       "75736  0.928053\n",
       "75737  0.914464\n",
       "75738  0.943409\n",
       "75739  0.941672\n",
       "\n",
       "[75740 rows x 1 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def buildReluNN():\n",
    "    model = Sequential([\n",
    "        Dense(32, activation = 'relu', input_dim = X.shape[1]),\n",
    "        Dense(16, activation = 'relu'),\n",
    "        Dense(8, activation = 'relu'),\n",
    "        Dense(4, activation = 'relu'),\n",
    "        Dense(1, activation = 'sigmoid'),\n",
    "    ])\n",
    "    optimizer = SGD(learning_rate=0.1)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['MSE'])\n",
    "    return(model)\n",
    "\n",
    "estimator = KerasRegressor(model=buildReluNN, epochs=200, batch_size=1000, verbose=1)\n",
    "history = estimator.fit(X_train, y_train, validation_data=(X_test.astype('float'), y_test))\n",
    "y_train_pred = estimator.predict(X_train)\n",
    "y_test_pred = estimator.predict(X_test)\n",
    "pd.DataFrame(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030162438788755852\n"
     ]
    }
   ],
   "source": [
    "MSE = sum((y_test_pred - y_test)**2)/y_test.size\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34909416375872704\n"
     ]
    }
   ],
   "source": [
    "# Fraction of Variance Unexplained\n",
    "FVU = MSE/np.var(y_test)\n",
    "print(FVU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildHPmodel(hp):\n",
    "  model= Sequential([\n",
    "      Dense(32, activation = 'relu', input_dim = X.shape[1]),\n",
    "      Dense(\n",
    "              units=hp.Int(\"units\", min_value=8, max_value=24, step=4),\n",
    "              activation= 'relu',\n",
    "      ),\n",
    "      Dense(\n",
    "              units=hp.Int(\"units\", min_value=4, max_value=16, step=4),\n",
    "              activation= 'relu',\n",
    "      ),\n",
    "      Dense(\n",
    "              units=hp.Int(\"units\", min_value=1, max_value=8, step=4),\n",
    "              activation= 'relu',\n",
    "      ),\n",
    "      Dense(1, activation = 'sigmoid')\n",
    "  ])\n",
    "\n",
    "  optimizer = SGD(learning_rate=0.3)\n",
    "  model.compile(optimizer=optimizer, loss='mse', metrics=['MSE'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 Complete [00h 01m 21s]\n",
      "val_loss: 0.02869807370007038\n",
      "\n",
      "Best val_loss So Far: 0.02869807370007038\n",
      "Total elapsed time: 00h 12m 06s\n"
     ]
    }
   ],
   "source": [
    "hp = keras_tuner.HyperParameters()\n",
    "\n",
    "tuner = keras_tuner.GridSearch(\n",
    "    hypermodel=buildHPmodel,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=10,\n",
    "    seed=0,\n",
    "    executions_per_trial=1,\n",
    "    hyperparameters=hp,\n",
    "    tune_new_entries=True,\n",
    "    allow_new_entries=True,\n",
    "    max_consecutive_failed_trials=3,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "X_train = np.asarray(X_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "X_test = np.asarray(X_test).astype(np.float32)\n",
    "y_test = np.asarray(y_test).astype(np.float32)\n",
    "tuner.search(X_train, y_train, epochs=5, validation_data=(X_test, y_test), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in .\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "units: 24\n",
      "Score: 0.02869807370007038\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units: 14\n",
      "Score: 0.028774499893188477\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "units: 22\n",
      "Score: 0.028802407905459404\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units: 8\n",
      "Score: 0.02884363941848278\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units: 10\n",
      "Score: 0.02884863130748272\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "units: 16\n",
      "Score: 0.02886430360376835\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units: 18\n",
      "Score: 0.02886887826025486\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units: 12\n",
      "Score: 0.028917666524648666\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "units: 20\n",
      "Score: 0.028966402634978294\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HPmodel(hp):\n",
    "    model = Sequential([\n",
    "        Dense(hp.Int(\"units\", min_value=32, max_value=64, step=2), activation = hp.Choice(\"activation\", [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"]), input_dim = X.shape[1]),\n",
    "        Dense(hp.Int(\"units\", min_value=32, max_value=256, step=8), activation = hp.Choice(\"activation\", [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"])),\n",
    "        Dense(hp.Int(\"units\", min_value=32, max_value=128, step=4), activation = hp.Choice(\"activation\", [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"])),\n",
    "        Dense(hp.Int(\"units\", min_value=32, max_value=64, step=4), activation = hp.Choice(\"activation\", [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"])),\n",
    "        Dense(1, activation = 'sigmoid'),\n",
    "    ])\n",
    "\n",
    "    learning_rate = hp.Float('lr', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    optimizer = SGD(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['MSE'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    HPmodel,\n",
    "    overwrite = True,\n",
    "    objective='val_loss',\n",
    "    max_trials = 20,\n",
    "    max_consecutive_failed_trials=3\n",
    ")\n",
    "tuner.search(np.array(X_train).astype('float32'), y_train, epochs=5, validation_data=(np.array(X_test).astype('float32'), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tuner.get_best_models()[0]\n",
    "print(model.summary())\n",
    "y_pred = model.predict(np.array(X_test).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = y_pred.reshape(75740,)\n",
    "MSE = sum((y_test_pred - y_test)**2)/y_test.size\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FVU = MSE/np.var(y_test)\n",
    "print(FVU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

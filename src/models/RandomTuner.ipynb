{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras_tuner\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset here\n",
    "file = open(\"../dataset/processed_reviews.json\", 'r', encoding='utf8')\n",
    "dataset_dict = json.load(file)\n",
    "df_raw = pd.DataFrame(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused features\n",
    "df = df_raw.copy(deep=True) #Do this so that I dont have to rerun the previous cell every time I make a change\n",
    "df.drop(columns=['firm','job_title'], inplace=True) #one hotting these would create too many features\n",
    "\n",
    "# Split up Date\n",
    "df['date'] = pd.to_datetime(df['date_review'])\n",
    "df['month'] = df['date'].dt.month.astype(str)\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Consider the length text inputs\n",
    "df['pros_length'] = df['pros'].apply(lambda x: len(x))\n",
    "df['cons_length'] = df['cons'].apply(lambda x: len(x))\n",
    "df.drop(columns=['headline', 'pros', 'cons'], inplace=True)\n",
    "\n",
    "# Encode 'current' as int\n",
    "df['current'] = (df['current'] == 'Current Employee').astype(int)\n",
    "\n",
    "# Min-max normalization\n",
    "scaler = MinMaxScaler()\n",
    "numeric_cols = df.select_dtypes(include=['int', 'float']).columns\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "#One hot encode\n",
    "one_hot_encoded = pd.get_dummies(df[['recommend', 'ceo_approv', 'outlook', 'month', 'duration']])\n",
    "df = pd.concat([df, one_hot_encoded], axis=1)\n",
    "df.drop(columns=['date', 'date_review', 'recommend', 'ceo_approv', 'outlook', 'month', 'duration'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (681651, 39) (681651,)\n",
      "Testing set shape: (75740, 39) (75740,)\n"
     ]
    }
   ],
   "source": [
    "#Spliting the data\n",
    "X = df.drop(columns=['overall_rating'])\n",
    "y = df['overall_rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HPmodel(hp):\n",
    "    model = Sequential([\n",
    "        Dense(hp.Int(\"units\", min_value=32, max_value=64, step=2), activation = hp.Choice(\"activation\", [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"]), input_dim = X.shape[1]),\n",
    "        Dense(hp.Int(\"units\", min_value=32, max_value=256, step=8), activation = hp.Choice(\"activation\", [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"])),\n",
    "        Dense(hp.Int(\"units\", min_value=32, max_value=128, step=4), activation = hp.Choice(\"activation\", [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"])),\n",
    "        Dense(hp.Int(\"units\", min_value=32, max_value=64, step=4), activation = hp.Choice(\"activation\", [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"])),\n",
    "        Dense(1, activation = 'sigmoid'),\n",
    "    ])\n",
    "\n",
    "    learning_rate = hp.Float('lr', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    optimizer = SGD(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['MSE'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 04m 38s]\n",
      "val_loss: 0.0335109680891037\n",
      "\n",
      "Best val_loss So Far: 0.030189480632543564\n",
      "Total elapsed time: 01h 47m 27s\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    HPmodel,\n",
    "    overwrite = True,\n",
    "    objective='val_loss',\n",
    "    max_trials = 20,\n",
    "    max_consecutive_failed_trials=3\n",
    ")\n",
    "tuner.search(np.array(X_train).astype('float32'), y_train, epochs=5, validation_data=(np.array(X_test).astype('float32'), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                2000      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9701 (37.89 KB)\n",
      "Trainable params: 9701 (37.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "2367/2367 [==============================] - 5s 2ms/step\n",
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 10 summary\n",
      "Hyperparameters:\n",
      "units: 50\n",
      "activation: relu\n",
      "lr: 0.006805387698117665\n",
      "Score: 0.030189480632543564\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "units: 56\n",
      "activation: tanh\n",
      "lr: 0.006542820504404195\n",
      "Score: 0.030377069488167763\n",
      "\n",
      "Trial 12 summary\n",
      "Hyperparameters:\n",
      "units: 62\n",
      "activation: tanh\n",
      "lr: 0.0023095567943677625\n",
      "Score: 0.03066808544099331\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "activation: tanh\n",
      "lr: 0.0017266468510148513\n",
      "Score: 0.030761418864130974\n",
      "\n",
      "Trial 19 summary\n",
      "Hyperparameters:\n",
      "units: 40\n",
      "activation: tanh\n",
      "lr: 0.00039868977350065565\n",
      "Score: 0.0335109680891037\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "units: 36\n",
      "activation: tanh\n",
      "lr: 0.0001113314039398187\n",
      "Score: 0.038207538425922394\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "units: 60\n",
      "activation: relu\n",
      "lr: 0.0001518632946972694\n",
      "Score: 0.04610494151711464\n",
      "\n",
      "Trial 17 summary\n",
      "Hyperparameters:\n",
      "units: 64\n",
      "activation: relu\n",
      "lr: 0.00012171195427975157\n",
      "Score: 0.04967649281024933\n",
      "\n",
      "Trial 18 summary\n",
      "Hyperparameters:\n",
      "units: 36\n",
      "activation: sigmoid\n",
      "lr: 0.007709359697494747\n",
      "Score: 0.0845789760351181\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "units: 44\n",
      "activation: sigmoid\n",
      "lr: 0.0004936498961125526\n",
      "Score: 0.08625704050064087\n"
     ]
    }
   ],
   "source": [
    "model = tuner.get_best_models()[0]\n",
    "print(model.summary())\n",
    "tuner.results_summary()\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_hp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030189490467440894\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(np.array(X_test).astype('float32'))\n",
    "y_test_pred = y_pred.reshape(75740,)\n",
    "MSE = sum((y_test_pred - y_test)**2)/y_test.size\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3494072371189498\n"
     ]
    }
   ],
   "source": [
    "FVU = MSE/np.var(y_test)\n",
    "print(FVU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=4,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    "    start_from_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = HPmodel(best_hp)\n",
    "history = best_model.fit(np.array(X_train).astype('float32'), y_train, validation_split = .15, batch_size = 1000, epochs = 200, callbacks = [early_stopping], verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2367/2367 [==============================] - 4s 2ms/step\n",
      "0.030023938220445422\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(np.array(X_test).astype('float32'))\n",
    "y_test_pred = y_pred.reshape(75740,)\n",
    "MSE = sum((y_test_pred - y_test)**2)/y_test.size\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34749116790658907\n"
     ]
    }
   ],
   "source": [
    "FVU = MSE/np.var(y_test)\n",
    "print(FVU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
